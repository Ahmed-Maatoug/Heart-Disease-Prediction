# -*- coding: utf-8 -*-
"""Project HearthDisease.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1v9_mBftlq6I78rkYQWTjru6sxfrnfuZT
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings("ignore", category=FutureWarning)

# Charger le dataset
df = pd.read_csv('/content/Data2.csv')

# 1. Compréhension de la structure du Dataset
# Dimensions du dataset
print("Dimensions du Dataset:", df.shape)

# Aperçu des données
print("\nAperçu des données:\n", df.head())

# Statistiques descriptives
print("\nStatistiques descriptives:\n", df.describe())

# Moyenne et médiane pour les colonnes numériques
mean_values = df.mean()
median_values = df.median()
print("\nMoyenne des colonnes numériques:\n", mean_values)
print("\nMédiane des colonnes numériques:\n", median_values)

# Vérification des types de données
print("\nTypes de données:\n", df.dtypes)

# 2. Visualisations préliminaires

# Histogrammes pour les colonnes numériques
df.hist(figsize=(15, 10), bins=20, color='skyblue', edgecolor='black')
plt.suptitle('Distributions des colonnes numériques')
plt.show()

# Boxplots pour les colonnes numériques
for col in df.columns:
    plt.figure(figsize=(8, 6))
    sns.boxplot(x=df[col], color='lightgreen')
    plt.title(f'Boxplot de {col}')
    plt.show()

# Heatmap de corrélation entre les variables numériques
plt.figure(figsize=(12, 8))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Matrice de Corrélation entre les variables numériques')
plt.show()

# 3. Analyser les relations entre certaines variables
# Par exemple, analyser la relation entre Age et Heart Disease (target)
plt.figure(figsize=(8, 6))
sns.boxplot(x=df['HeartDisease'], y=df['Age'], palette='Set2')
plt.title('Relation entre Age et la présence de maladies cardiaques')
plt.show()
plt.figure(figsize=(8, 6))
sns.boxplot(x=df['HeartDisease'], y=df['CholesterolLevel'], palette='Set2')
plt.title('Relation entre CholesterolLevel et la présence de maladies cardiaques')
plt.show()
plt.figure(figsize=(8, 6))
sns.boxplot(x=df['HeartDisease'], y=df['ChestPainType'], palette='Set2')
plt.title('Relation entre ChestPainType et la présence de maladies cardiaques')
plt.show()

# Vérifier les valeurs manquantes
print(df.isnull().sum())

# Remplacer les valeurs manquantes par la médiane pour chaque colonne numérique
df.fillna(df.median(), inplace=True)

# Suppression des valeurs aberrantes pour toutes les colonnes numériques
for column in df.select_dtypes(include=['float64', 'int64']).columns:
    # Calcul des quartiles et de l'IQR
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1

    # Définition des bornes inférieure et supérieure
    lower_bound = Q1 - 1.5 * IQR #(Interquartile Range)
    upper_bound = Q3 + 1.5 * IQR #(Interquartile Range)

    # Filtrage des données pour supprimer les valeurs aberrantes
    df = df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]

# Vérification après nettoyage des données
print("Dimensions après suppression des valeurs aberrantes:", df.shape)

from sklearn.preprocessing import StandardScaler

# Séparer les colonnes numériques et catégorielles
numerical_cols = ['Age', 'RestingBloodPressure', 'CholesterolLevel', 'FastingBloodSugar',
                  'RestingECG', 'MaxHeartRateAchieved',
                  'ExerciseInducedAngina', 'STDepression', 'STSlopeSTSlope',
                  'MajorVessels', 'Thalassemia']  # Liste des colonnes numériques

categorical_cols = ['Gender', 'ChestPainType']  # Liste des colonnes catégorielles

# Sélectionner les données numériques
X_numerical = df[numerical_cols]

# Normaliser les données numériques
scaler = StandardScaler()
X_numerical_normalized = scaler.fit_transform(X_numerical)

# Convertir les données normalisées en DataFrame
df_numerical_normalized = pd.DataFrame(X_numerical_normalized, columns=numerical_cols)

# Concaténer avec les données catégorielles
df_final = pd.concat([df_numerical_normalized, df[categorical_cols], df['HeartDisease']], axis=1)

# Vérifier la structure du DataFrame final
print(df_final.head())

# Imputation des valeurs manquantes
df_final['Gender'] = df_final['Gender'].fillna(df_final['Gender'].mode()[0])
df_final['ChestPainType'] = df_final['ChestPainType'].fillna(df_final['ChestPainType'].mode()[0])
df_final['HeartDisease'] = df_final['HeartDisease'].fillna(df_final['HeartDisease'].mode()[0])


# Vérifier la structure du DataFrame final
print(df_final.isnull().sum())  # Pour vérifier qu'il n'y a plus de NaN
print(df_final.head())  # Vérifier les premières lignes pour s'assurer que tout est correct

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score

# Séparer les caractéristiques (X) et la variable cible (y)
X = df_final.drop(columns=['HeartDisease'])
y = df_final['HeartDisease']

# Diviser les données en ensemble d'entraînement et de test (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialiser le modèle Random Forest
model = RandomForestClassifier()

# Entraîner le modèle
model.fit(X_train, y_train)

# Prédictions sur l'ensemble de test
y_pred = model.predict(X_test)

# Évaluation du modèle
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Classification Report:")
print(classification_report(y_test, y_pred))

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report
import sklearn

# Vérifier la version de Scikit-learn
print("Version de Scikit-learn :", sklearn.__version__)

# Définir le modèle RandomForest
rf = RandomForestClassifier(random_state=42)

# Définir les hyperparamètres à tester
param_grid = {
    'n_estimators': [50, 100, 200],  # Nombre d'arbres
    'max_depth': [None, 10, 20, 30],  # Profondeur des arbres
    'min_samples_split': [2, 5, 10],  # Nombre minimum d'échantillons pour diviser un noeud
    'min_samples_leaf': [1, 2, 4],  # Nombre minimum d'échantillons pour une feuille
    'max_features': ['sqrt', 'log2'],  # Nombre de caractéristiques à prendre en compte
    'bootstrap': [True, False]  # Bootstrap ou non
}

# Mettre en place GridSearchCV
grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)

# Exécuter la recherche sur grille
grid_search.fit(X_train, y_train)

# Afficher les meilleurs paramètres
print("Meilleurs paramètres : ", grid_search.best_params_)

# Appliquer le modèle avec les meilleurs paramètres sur le test
best_rf = grid_search.best_estimator_
y_pred = best_rf.predict(X_test)

# Évaluation du modèle avec un rapport de classification
print(classification_report(y_test, y_pred))

from imblearn.over_sampling import SMOTE
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

# Diviser les données en caractéristiques et étiquettes
X = df.drop("HeartDisease", axis=1)
y = df["HeartDisease"]

# Diviser les données en ensembles d'entraînement et de test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Appliquer SMOTE pour rééquilibrer les classes dans l'ensemble d'entraînement
smote = SMOTE(random_state=42)
X_train_res, y_train_res = smote.fit_resample(X_train, y_train)

# Créer et entraîner le modèle RandomForest avec les données rééquilibrées
rf = RandomForestClassifier(bootstrap=True, max_depth=None, max_features='sqrt',
                            min_samples_leaf=4, min_samples_split=2, n_estimators=100, random_state=42)
rf.fit(X_train_res, y_train_res)

# Évaluer le modèle sur l'ensemble de test
y_pred = rf.predict(X_test)

# Afficher les résultats
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Classification Report:")
print(classification_report(y_test, y_pred))

from imblearn.pipeline import Pipeline
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier

# Définir le pipeline
smote = SMOTE(random_state=42)
rf = RandomForestClassifier(random_state=42)

# Paramètres pour la recherche en grille
param_grid = {
    'rf__n_estimators': [100, 200],
    'rf__max_depth': [None, 10, 20],
    'rf__min_samples_split': [2, 5],
    'rf__min_samples_leaf': [1, 2],
    'rf__max_features': ['sqrt', 'log2']
}

# Créer le pipeline avec SMOTE et RandomForest
pipeline = Pipeline(steps=[('smote', smote), ('rf', rf)])

# Configurer la recherche en grille avec validation croisée
grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1)

# Effectuer la recherche en grille
grid_search.fit(X_train, y_train)

# Meilleurs paramètres et score
print(f"Best parameters: {grid_search.best_params_}")
print(f"Best cross-validation score: {grid_search.best_score_}")
# Évaluation du modèle avec un rapport de classification
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Classification Report:")
print(classification_report(y_test, y_pred))